
# KubeFlow-HandsOn

## Overview

This repository provides a hands-on example of creating and deploying a simple machine learning pipeline using [Kubeflow Pipelines](https://www.kubeflow.org/docs/components/pipelines/). The primary goal is to demonstrate how to define pipeline components, compile them into a workflow, and execute them within a Kubeflow environment.

<img src="https://github.com/Abeshith/KubeFlow-HandsOn/blob/main/Outputs/workflow.png?raw=true" alt="Kubeflow Workflow" width="100%" height="700px">


## Repository Structure

- **`pipeline.py`**: Defines the pipeline components and assembles them into a workflow using the Kubeflow Pipelines SDK.
- **`kubeflow_pipeline.yaml`**: The compiled pipeline in YAML format, ready for deployment to the Kubeflow Pipelines UI or API.
- **`Outputs/`**: Directory intended to store outputs generated by pipeline runs.

## Prerequisites

Before running the pipeline, ensure the following:

- A functioning Kubeflow environment is set up. You can refer to the [Kubeflow documentation](https://www.kubeflow.org/docs/started/installing-kubeflow/) for installation guidance.
- The Kubeflow Pipelines SDK is installed:

```bash
pip install kfp
```

## Defining the Pipeline

In `pipeline.py`, the pipeline is constructed using the Kubeflow Pipelines SDK. The pipeline consists of the following components:

1. **Data Preprocessing**: Handles data cleaning and preparation tasks.
2. **Model Training**: Trains a machine learning model using the preprocessed data.
3. **Model Evaluation**: Evaluates the trained model's performance on a test dataset.

Each component is defined as a Python function and converted into a pipeline step using the `@dsl.component` decorator. The components are then connected in a logical sequence to form the complete pipeline.

## Compiling the Pipeline

To compile the pipeline into a YAML file suitable for deployment:

```bash
python pipeline.py
```

This command generates the `kubeflow_pipeline.yaml` file, which contains the pipeline definition in a format that Kubeflow can interpret.

## Deploying the Pipeline

To deploy and run the pipeline:

1. **Access the Kubeflow Pipelines UI**: Navigate to the Kubeflow Pipelines dashboard in your browser.
2. **Upload the Pipeline**:
   - Click on "Upload Pipeline".
   - Provide a name for the pipeline.
   - Upload the `kubeflow_pipeline.yaml` file.
3. **Run the Pipeline**:
   - After uploading, select the pipeline from the list.
   - Click on "Create Run".
   - Configure any necessary parameters and start the run.

## Monitoring Pipeline Runs

Once the pipeline is running:

- **View Progress**: The Kubeflow Pipelines UI provides a visual representation of the pipeline's progress, showing each component's status.
- **Access Logs**: Click on individual components to view their execution logs, which can help in debugging or understanding the pipeline's behavior.
- **Review Outputs**: Outputs from each component can be accessed through the UI or found in the `Outputs/` directory if configured to store results there.

## Customizing the Pipeline

You can modify `pipeline.py` to:

- Add new components or steps.
- Change the logic within existing components.
- Adjust the sequence or dependencies between components.

After making changes, recompile the pipeline:

```bash
python pipeline.py
```

Then, re-upload the updated `kubeflow_pipeline.yaml` to the Kubeflow Pipelines UI to deploy the new version.

## Conclusion

This repository serves as a foundational example for understanding and working with Kubeflow Pipelines. By exploring and modifying the provided pipeline, you can gain hands-on experience in building and managing machine learning workflows in a Kubernetes-based environment.
